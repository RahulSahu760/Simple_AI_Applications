# -*- coding: utf-8 -*-
"""STTandTTS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxbxEKov5vDKU_QEkwmcXVrnHFOmlarg
"""

!pip install torch torchaudio
!pip install git+https://github.com/snakers4/silero-models.git

import torch # main pytorch lib used to load and run models
import torchaudio # used to handle audio files (read, preprocess,etc)
from glob import glob

STText = ''

#loading silero stt model
def load_stt_model():
    device = torch.device('cpu')
    # model - neural network which will convert audio to tokens
    # decoder - converts tokens(model output tokens) to text
    # utils - preprocess audio and prepare model input
    model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language = 'en', device=device)
    return model, decoder, utils

model, decoder, utils = load_stt_model()
# read_batch - reads audio files and preprocesses them
# split_into_batches - splits audio files into batches
# read_audio - reads audio files
# prepare_model_input - prepares model input
(read_batch, split_into_batches, read_audio, prepare_model_input) = utils


def speech_to_text(audio_path):
    test_files = glob(audio_path)
    batches = split_into_batches(test_files, batch_size = 1)
    input = prepare_model_input(read_batch(batches[0]), device = torch.device('cpu'))
    output = model(input)

    return decoder(output[0].cpu())

STText = speech_to_text('audio.wav')
print(STText)

"""# Processing by llama cpp from Hugging face

"""

!pip install llama-cpp-python huggingface-hub

from huggingface_hub import hf_hub_download

model_path = hf_hub_download(
    repo_id="TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
    filename="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
)
print("Model downloaded to:", model_path)

from llama_cpp import Llama

llm = Llama(
    model_path=model_path,
    n_ctx=2048,
    n_threads=8  # You can tune this based on your environment
)

system_prompt = "You are a helpful voice assistant."
user_prompt = STText

prompt = f"<|system|>\n{system_prompt}</s>\n<|user|>\n{user_prompt}</s>\n<|assistant|>"

output = llm(prompt, max_tokens=256, temperature=0.7, stop=["</s>"])
reply = output['choices'][0]['text'].strip()
print(reply)

"""# TEXT TO SPEECH with Silero

"""

import torch
import os
import IPython.display as ipd
import numpy as np
import scipy.io.wavfile

language = 'en'
model_id = 'v3_en'
sample_rate = 8000
speaker = 'en_0'
device = torch.device('cpu')

model, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models',
                                     model='silero_tts',
                                     language=language,
                                     speaker=model_id)
model.to(device)  # gpu or cpu

audio = model.apply_tts(text=reply,
                        speaker=speaker,
                        sample_rate=sample_rate)
audio_np = np.array(audio)

# Save to a .wav file
output_path = '/output.wav'  # Google Colab root directory
scipy.io.wavfile.write(output_path, sample_rate, audio_np)

# Play the audio in Colab
ipd.Audio(output_path)